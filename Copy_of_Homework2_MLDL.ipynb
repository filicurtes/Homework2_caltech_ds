{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Homework2-MLDL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davibuss/Homework2-Caltech101/blob/master/Copy_of_Homework2_MLDL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9QcGnGPdX2C",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9O3aM3Tb28q",
        "colab_type": "code",
        "outputId": "8751fd6c-59f2-4109-852d-0f1939f3f04b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\n",
        "!pip install imutils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.3.1\n",
            "  Using cached https://files.pythonhosted.org/packages/88/95/90e8c4c31cfc67248bf944ba42029295b77159982f532c5689bcfe4e9108/torch-1.3.1-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.1) (1.18.3)\n",
            "\u001b[31mERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you'll have torch 1.3.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "Successfully installed torch-1.3.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.18.3)\n",
            "Collecting torch==1.4.0\n",
            "  Using cached https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.3.1\n",
            "    Uninstalling torch-1.3.1:\n",
            "      Successfully uninstalled torch-1.3.1\n",
            "Successfully installed torch-1.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow-SIMD in /usr/local/lib/python3.6/dist-packages (7.0.0.post3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.6/dist-packages (0.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo942LMOdlh4",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DokFOdD1dJEl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6634c4c5-db61-4b1b-89d2-d10834a01804"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import os.path\n",
        "import sys\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "from torchvision.datasets import VisionDataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDLJuIXK_vh",
        "colab_type": "text"
      },
      "source": [
        "**Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5PkYfqfK_SA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 102 # 101 + 1: There is am extra Background class that should be removed \n",
        "\n",
        "BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 1e-3            # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 100     # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 90       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwii0TBHvzh",
        "colab_type": "text"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUDdw4j2H0Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                   # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                   # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalizes tensor with mean and standard deviation\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVKRPDyYRGSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "    \n",
        "   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIAIwyfFRQaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qYIHPzYLY7i",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfVq_uDHLbsR",
        "colab_type": "code",
        "outputId": "7da9dcb2-afdd-4ba0-8066-f6db725e3f1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Clone github repository with data\n",
        "if not os.path.isdir('./Caltech101'):\n",
        "  !git clone https://github.com/davibuss/Homework2-Caltech101.git\n",
        "  !mv 'Homework2-Caltech101' 'Caltech101'\n",
        "\n",
        "DATA_DIR = 'Caltech101/101_ObjectCategories'\n",
        "from Caltech101.caltech_dataset import Caltech\n",
        "\n",
        "\n",
        "# Prepare Pytorch train/test Datasets\n",
        "train_dataset = Caltech(DATA_DIR, split='train',  transform=train_transform)\n",
        "test_dataset = Caltech(DATA_DIR, split='test', transform=eval_transform)\n",
        "print(type(train_dataset))\n",
        "\n",
        "\"\"\"\n",
        "train_len = int(train_dataset.__len__() * 0.5)\n",
        "val_len = int(train_dataset.__len__() * 0.5)\n",
        "train_indexes = np.arange(train_dataset.__len__())\n",
        "print(train_indexes)\n",
        "train_labels = np.empty(train_dataset.__len__(), dtype=int)\n",
        "\n",
        "\n",
        "for index in train_indexes:\n",
        "  train_labels[index] = train_dataset.__getitem__(index)[1]\n",
        "print(train_labels)  \n",
        "\n",
        "train_indexes, val_indexes, _, _ = train_test_split(train_indexes, train_labels, test_size=0.5, random_state=42, stratify=train_labels)\n",
        "\n",
        "\"\"\"\n",
        "validation_split = 0.5\n",
        "dataset_size = len(train_dataset)\n",
        "#print(dataset_size)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if 1 :\n",
        "    np.random.seed(1337)\n",
        "    np.random.shuffle(indices)\n",
        "train_indexes, val_indexes = indices[split:], indices[:split]\n",
        "print(train_indexes)\n",
        "print(val_indexes)\n",
        "print(type(train_indexes))\n",
        "#train_indexes = SubsetRandomSampler(train_indices)\n",
        "#val_indexes = SubsetRandomSampler(valid_indices)\n",
        "#train_indexes, val_indexes = \n",
        "#print(type(train_indexes))\n",
        "#print(train_indexes)\n",
        "\n",
        "\n",
        "val_dataset = Subset(train_dataset, val_indexes)\n",
        "train_dataset = Subset(train_dataset, train_indexes)\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "print('Valid Dataset: {}'.format(len(val_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'Caltech101.caltech_dataset.Caltech'>\n",
            "[3249, 2705, 2563, 5560, 3607, 3586, 2192, 1258, 1965, 5166, 3827, 1600, 1986, 1308, 2152, 2768, 876, 1141, 769, 5634, 2846, 3546, 639, 4042, 413, 5533, 4799, 214, 2762, 4241, 4522, 4450, 3854, 5078, 755, 4617, 188, 863, 4420, 5654, 2086, 1982, 5044, 4360, 5203, 2325, 1012, 5333, 4598, 2170, 4892, 4796, 5435, 6, 4174, 1074, 2619, 2931, 5413, 1697, 3843, 952, 2227, 948, 4668, 238, 5450, 2037, 2926, 3009, 1811, 5601, 3322, 2753, 2128, 5553, 4806, 75, 42, 3075, 28, 5116, 447, 2133, 5065, 2882, 298, 2829, 209, 1551, 901, 1328, 3751, 3798, 1223, 4052, 2191, 2925, 2421, 5709, 1363, 2979, 4169, 886, 5339, 1884, 268, 3325, 2290, 1711, 2646, 429, 3876, 5603, 469, 1383, 3984, 4215, 3193, 5584, 185, 3320, 3721, 1815, 358, 1723, 205, 2901, 5147, 1378, 3755, 3398, 3525, 3674, 2009, 1435, 1781, 5237, 4495, 4033, 4721, 1109, 4630, 433, 3220, 2326, 2725, 224, 4475, 1298, 2779, 1036, 1369, 3399, 4203, 2787, 4415, 524, 5370, 1429, 3633, 31, 810, 3520, 718, 3364, 2108, 328, 2116, 5053, 5342, 3205, 1073, 4939, 4212, 1462, 3257, 1457, 4300, 1677, 3805, 347, 39, 776, 3960, 3429, 4442, 5661, 3177, 2318, 4692, 5321, 5056, 5334, 2946, 3268, 2815, 3558, 983, 1110, 1791, 5506, 5395, 1023, 666, 154, 5728, 4546, 2351, 3495, 5326, 615, 114, 4549, 3541, 1909, 2474, 2092, 1326, 4099, 2883, 5630, 4674, 4834, 5724, 1975, 2715, 1718, 743, 2974, 5706, 1125, 3712, 4844, 1861, 3884, 4349, 2612, 4363, 3476, 3484, 2886, 5257, 4945, 3486, 1808, 4422, 2868, 1119, 1159, 619, 330, 310, 5029, 3019, 3151, 5544, 4373, 1484, 2182, 4508, 1611, 2277, 3011, 2331, 2877, 2015, 4077, 62, 2596, 4805, 1788, 3477, 4202, 510, 2329, 3435, 3090, 5567, 383, 1276, 3601, 1210, 1812, 2845, 697, 66, 3475, 537, 5583, 2072, 280, 2731, 2927, 2328, 3536, 2533, 5224, 1823, 1436, 94, 3631, 1260, 3781, 2704, 2238, 1349, 4650, 2764, 2048, 3150, 4048, 285, 711, 2472, 4563, 1765, 3908, 2493, 3481, 575, 277, 3583, 3701, 1382, 301, 4702, 1866, 3288, 3156, 2013, 1289, 63, 2641, 1686, 918, 4256, 3392, 3932, 5529, 1904, 1134, 685, 4155, 3801, 2919, 4496, 4860, 1329, 1796, 132, 3181, 4995, 171, 2230, 2419, 1740, 5034, 959, 5741, 41, 3023, 4881, 2831, 4564, 5518, 1384, 1364, 1340, 1221, 4993, 4482, 349, 122, 1398, 1745, 2887, 724, 5519, 3553, 1111, 3989, 2864, 3186, 3621, 5186, 1983, 220, 1939, 5112, 5752, 1365, 4530, 5252, 633, 4151, 832, 3653, 4467, 802, 1702, 4980, 262, 2981, 5080, 5052, 78, 1408, 3024, 1647, 4988, 217, 4597, 2536, 4164, 3514, 2478, 5547, 3509, 142, 219, 3670, 5624, 3778, 5689, 425, 3639, 1592, 3963, 4777, 463, 3382, 1286, 4135, 319, 1002, 5302, 5361, 2893, 2814, 1585, 3291, 638, 1076, 1024, 4846, 4908, 3184, 1730, 3285, 4636, 582, 4943, 736, 2346, 2237, 4569, 964, 4672, 3838, 989, 2736, 1961, 3741, 1046, 5020, 4849, 3188, 1588, 974, 5760, 2220, 2631, 5776, 600, 896, 1246, 1650, 5136, 807, 3060, 4794, 3120, 3503, 4807, 2436, 5201, 4377, 540, 707, 861, 417, 696, 467, 2711, 869, 1655, 1822, 24, 5364, 4484, 461, 96, 4981, 4412, 3691, 4393, 2055, 1925, 411, 3573, 3474, 343, 82, 4000, 258, 4081, 3680, 4838, 5243, 2269, 2202, 2260, 5001, 2556, 4483, 4996, 331, 3521, 1804, 3119, 2135, 2544, 4998, 3362, 1173, 1906, 1951, 1454, 4953, 3772, 3196, 2454, 4890, 3167, 741, 1949, 993, 3051, 5474, 4708, 2432, 1164, 4612, 1445, 3941, 1621, 1179, 1706, 2861, 3489, 165, 2229, 5576, 1222, 865, 265, 579, 1519, 1287, 1956, 4946, 4186, 845, 954, 1161, 1242, 1833, 4097, 3359, 1847, 1910, 3436, 2465, 1048, 2052, 1607, 5134, 2287, 1856, 4757, 1642, 3065, 3561, 3157, 3066, 5405, 5063, 5396, 5175, 2740, 1178, 1806, 2165, 4620, 236, 5480, 3604, 4989, 4312, 3078, 523, 4017, 693, 3200, 129, 5091, 1434, 5778, 3597, 5151, 950, 665, 962, 3710, 3441, 4314, 4985, 221, 1770, 4346, 1691, 2990, 4015, 2129, 4558, 4150, 1835, 2348, 192, 2915, 1346, 494, 2040, 2370, 1215, 4007, 4823, 5023, 2139, 1537, 4673, 1375, 5226, 2989, 5206, 1839, 2520, 3195, 3449, 3811, 4141, 2029, 2620, 2291, 5514, 4271, 2079, 1807, 1162, 2218, 3687, 5041, 3619, 5737, 2797, 1463, 1372, 3420, 808, 2231, 4345, 251, 2625, 4242, 2758, 3406, 4992, 4934, 4587, 3064, 4711, 5421, 4340, 1385, 5210, 341, 3100, 5715, 2993, 4941, 2943, 2786, 5497, 990, 4652, 2050, 2585, 4923, 1675, 1114, 3519, 1301, 61, 4802, 2368, 926, 5062, 5587, 4079, 5612, 5445, 1876, 4756, 5189, 4940, 2366, 2017, 4960, 3265, 939, 121, 4073, 1753, 2403, 4759, 4751, 1268, 5415, 3319, 965, 1505, 4621, 2241, 4889, 4665, 3002, 2338, 667, 4283, 4296, 731, 3298, 1095, 2597, 1754, 547, 4425, 2281, 2415, 2535, 5456, 5733, 2352, 2913, 113, 3815, 502, 2210, 1321, 1544, 4520, 2587, 2501, 3636, 5678, 56, 5779, 1064, 5016, 1203, 5214, 180, 1654, 994, 4358, 5200, 2456, 4791, 4642, 3207, 3652, 797, 5668, 2510, 5434, 2686, 2726, 1633, 468, 3074, 873, 5495, 4224, 985, 4832, 2616, 906, 1317, 4928, 3954, 3745, 2735, 1402, 2937, 2650, 3283, 1526, 5513, 3244, 5764, 4189, 1707, 1418, 2492, 1938, 3650, 1518, 244, 1266, 9, 527, 3042, 1917, 1219, 2568, 202, 2243, 4488, 791, 4104, 732, 631, 4049, 1586, 5771, 830, 2529, 1536, 904, 3004, 3950, 1228, 2729, 3314, 663, 4669, 3059, 1559, 5720, 1510, 1313, 4594, 525, 3618, 719, 3279, 3556, 2920, 3874, 3753, 640, 5523, 5458, 3106, 5515, 710, 5323, 4610, 5658, 5383, 3930, 1715, 4913, 1160, 4423, 5488, 3899, 140, 211, 2853, 824, 2907, 4163, 3330, 3303, 1772, 1649, 4591, 1124, 4741, 2162, 5365, 3409, 5534, 1981, 2538, 3243, 5753, 2738, 5765, 4699, 5419, 4462, 543, 3517, 1417, 4932, 4762, 3136, 749, 1663, 4318, 1453, 5209, 1475, 1908, 2320, 4035, 1577, 1066, 4758, 2008, 2832, 611, 1390, 4749, 2296, 4294, 4460, 1973, 1829, 729, 394, 4357, 5265, 1263, 4092, 1728, 456, 679, 2198, 3910, 5439, 3433, 1233, 1226, 3904, 3534, 1761, 1406, 2423, 3385, 1952, 5763, 717, 3925, 2793, 1497, 4878, 2637, 5123, 5106, 5606, 581, 1499, 4750, 2593, 5485, 1967, 1863, 2130, 5072, 3824, 850, 1746, 3620, 2422, 3961, 2719, 488, 2851, 587, 1513, 5467, 4255, 2547, 3173, 3761, 4895, 694, 3985, 4615, 2623, 5391, 5537, 2184, 4969, 1578, 674, 5122, 4458, 3296, 3226, 58, 4429, 2852, 3762, 4829, 283, 5169, 739, 4028, 2571, 3255, 5686, 1259, 2678, 955, 1970, 1149, 835, 5300, 4523, 2580, 2084, 97, 5769, 4117, 2206, 5165, 434, 3804, 4726, 4551, 5633, 176, 4930, 2988, 5464, 2168, 4273, 969, 5697, 1498, 4631, 5462, 279, 1371, 1506, 720, 2408, 256, 748, 4999, 827, 3434, 2642, 1386, 1790, 207, 5117, 33, 2579, 88, 2267, 4258, 5075, 3872, 4109, 2978, 5543, 5095, 5718, 2064, 3529, 4740, 5324, 3992, 144, 4059, 2391, 3204, 852, 1148, 3692, 2163, 2479, 1081, 4268, 5521, 2960, 828, 174, 4173, 5199, 2027, 1471, 5666, 3067, 2262, 5240, 4387, 4536, 4783, 934, 3704, 5520, 4338, 5019, 3198, 5580, 5346, 5204, 5615, 3472, 1274, 2289, 1087, 1140, 4639, 2131, 2233, 2527, 4055, 5018, 377, 332, 304, 2359, 3658, 4632, 2514, 4223, 1610, 1206, 2323, 1021, 3711, 2891, 2613, 3562, 5068, 1722, 4521, 342, 65, 4472, 991, 5345, 3003, 2215, 2065, 569, 4983, 4384, 3141, 765, 2664, 472, 2358, 1362, 4510, 5179, 267, 794, 1783, 3792, 2532, 1596, 3901, 204, 1452, 3415, 4324, 2247, 5561, 4435, 814, 5727, 4047, 4113, 350, 1579, 3606, 4247, 3758, 1217, 5729, 3383, 412, 3147, 3793, 163, 3543, 5232, 2909, 4436, 4262, 4784, 1309, 5293, 5491, 1433, 2010, 5609, 1247, 5060, 2361, 930, 4329, 2606, 3252, 2031, 3130, 2, 3312, 225, 4887, 4380, 2936, 1485, 787, 4553, 2567, 4707, 5193, 5185, 4198, 1920, 2132, 4709, 1511, 2446, 3016, 596, 2464, 4098, 3714, 5632, 3886, 2021, 108, 2955, 1699, 1296, 2412, 2751, 988, 1330, 4818, 2477, 624, 4909, 4149, 4936, 2034, 2867, 2046, 4843, 4370, 2293, 2945, 3187, 5174, 3377, 112, 3043, 5131, 2667, 4935, 3635, 4780, 3865, 1200, 1138, 2309, 4734, 4322, 1380, 612, 3611, 5492, 30, 3379, 892, 653, 4348, 2187, 5730, 1886, 1782, 1072, 2744, 5568, 5469, 18, 1570, 3782, 3270, 5540, 2266, 5504, 1632, 2186, 4115, 1732, 5220, 675, 4020, 120, 1010, 5283, 4505, 3083, 2235, 2645, 2298, 2662, 2771, 5236, 2387, 3640, 227, 1693, 2598, 3203, 3694, 380, 3856, 3469, 2696, 981, 3727, 2503, 3820, 5031, 5411, 4288, 4426, 3526, 700, 1995, 5259, 2747, 3580, 768, 3395, 3231, 1865, 1848, 486, 2807, 3376, 2149, 2496, 1926, 4159, 55, 3563, 5376, 2207, 4470, 2800, 5281, 5144, 326, 2954, 2862, 3219, 1357, 1420, 1571, 4580, 14, 1749, 80, 3878, 5743, 2724, 539, 1500, 1028, 1344, 903, 3935, 1685, 1821, 2728, 2626, 4977, 160, 3155, 5499, 4024, 374, 5150, 1976, 4272, 2781, 4533, 825, 698, 1065, 4331, 405, 5581, 3456, 4478, 3693, 1202, 2217, 1937, 0, 4328, 4234, 4637, 3035, 4310, 3806, 4464, 1768, 3660, 3539, 4316, 2673, 3906, 1080, 150, 254, 1240, 1271, 1085, 1542, 3773, 1955, 1189, 2805, 2522, 646, 5676, 4781, 1392, 3861, 975, 4364, 3676, 5149, 2894, 1594, 908, 2706, 5734, 3750, 495, 4710, 3185, 1565, 141, 2324, 3790, 960, 609, 4964, 4469, 2854, 2932, 4715, 761, 1762, 477, 3630, 2150, 627, 2096, 5412, 3581, 692, 102, 5309, 1176, 2025, 1558, 5432, 104, 2714, 3702, 2844, 5256, 4057, 1999, 187, 93, 5599, 5097, 4933, 3277, 940, 5284, 3414, 1727, 2712, 3920, 5135, 3113, 4034, 3557, 649, 522, 2707, 1744, 1793, 2674, 3228, 2453, 5296, 2774, 3461, 661, 3275, 3857, 3850, 4862, 4956, 941, 1913, 3233, 3071, 5677, 5511, 3452, 4307, 1440, 2681, 2555, 1341, 4011, 3717, 2000, 3367, 5637, 2924, 390, 4576, 4731, 3642, 3373, 1003, 444, 2475, 1902, 4152, 3502, 966, 2070, 5667, 212, 1261, 4728, 5410, 2972, 1150, 3998, 3769, 2777, 4528, 3990, 4004, 218, 511, 5228, 1438, 2865, 632, 2648, 2244, 3175, 1844, 1318, 3644, 3046, 4379, 2841, 4342, 3335, 1112, 110, 359, 3055, 1797, 2354, 4622, 3816, 938, 2655, 291, 148, 2732, 5397, 3041, 2594, 3896, 5436, 5744, 1135, 4874, 2447, 2833, 259, 2778, 2221, 782, 1345, 1136, 3665, 2651, 764, 2056, 5267, 3977, 164, 1283, 2470, 4172, 3361, 4386, 2672, 5269, 1643, 870, 641, 4114, 4382, 5750, 2058, 4614, 5400, 4771, 2694, 2480, 191, 5430, 2141, 728, 2792, 5447, 4071, 1700, 5102, 1820, 357, 913, 851, 4184, 2386, 4350, 1659, 5591, 3860, 5614, 2043, 2427, 4183, 306, 2001, 410, 4431, 3381, 2156, 2770, 1400, 937, 5640, 3487, 2621, 3952, 3342, 3323, 625, 3013, 3315, 3084, 226, 5291, 1922, 2702, 2693, 1448, 3967, 5042, 1059, 229, 584, 2307, 4211, 512, 2649, 1231, 4821, 4139, 2910, 5369, 3266, 1555, 4545, 5564, 3634, 2951, 544, 680, 2734, 3661, 1492, 3612, 756, 2372, 534, 1425, 1894, 4538, 3578, 3582, 1279, 1735, 123, 81, 4698, 3959, 2063, 3432, 1495, 4537, 3423, 5120, 3567, 59, 5320, 5098, 1158, 4600, 4720, 5188, 2948, 2012, 4647, 4162, 2652, 3496, 831, 3724, 5501, 1535, 2032, 5298, 5245, 4919, 4876, 4065, 5035, 4863, 2161, 5294, 3183, 4089, 303, 1205, 3919, 3297, 4502, 1802, 668, 4518, 2518, 2429, 654, 3672, 2557, 3808, 3705, 3442, 428, 4653, 4906, 3453, 5055, 4463, 4819, 2534, 1281, 1895, 2629, 4770, 3142, 846, 5394, 549, 1984, 2360, 4263, 4013, 5167, 925, 528, 995, 5025, 817, 2484, 4961, 1155, 670, 3028, 5468, 2796, 387, 3685, 760, 5198, 5406, 4368, 439, 5551, 1907, 49, 2123, 4302, 1115, 4735, 2059, 2811, 1407, 3056, 605, 4107, 5671, 2817, 5711, 4833, 542, 3174, 3287, 4627, 2528, 2827, 4503, 648, 4869, 4957, 3388, 3579, 862, 778, 3864, 3575, 5266, 3566, 352, 3138, 5287, 684, 3235, 2908, 2394, 677, 1623, 1403, 2828, 133, 3742, 664, 5691, 5009, 5742, 2431, 533, 2609, 4837, 589, 2630, 3206, 4842, 5307, 4590, 366, 2572, 3402, 1977, 716, 751, 1634, 4453, 4, 4776, 146, 2353, 709, 4227, 5645, 2178, 1692, 346, 3410, 706, 574, 4454, 478, 3988, 4419, 3111, 726, 3695, 2411, 4148, 2120, 2073, 4824, 855, 5701, 362, 253, 367, 1527, 1778, 5408, 406, 2754, 4044, 1682, 1670, 1208, 3834, 5325, 617, 5389, 5494, 4648, 26, 3494, 3374, 2274, 247, 4604, 3540, 2684, 5735, 3948, 4421, 5657, 785, 1483, 2884, 5057, 2151, 4214, 1172, 4962, 3651, 1169, 4586, 147, 5007, 3812, 2821, 1924, 1029, 3638, 1545, 2971, 968, 3479, 3971, 1620, 514, 730, 5613, 3396, 5457, 3199, 4619, 4556, 2024, 5327, 3215, 3290, 5481, 1427, 3089, 2089, 3022, 2588, 2521, 2840, 3260, 2969, 2434, 5401, 5681, 2987, 3966, 1893, 5094, 3444, 440, 1755, 249, 2392, 5777, 1316, 5723, 775, 3082, 3133, 871, 5366, 409, 177, 5235, 4808, 551, 2542, 5502, 3897, 4490, 4511, 1377, 5687, 4717, 1751, 2310, 1972, 1250, 3144, 1401, 5694, 507, 1322, 3697, 4540, 2340, 1852, 354, 943, 2246, 70, 2785, 3241, 4800, 3, 2460, 1736, 2450, 2818, 4677, 442, 647, 5289, 3079, 2080, 4517, 5180, 5045, 1288, 1201, 3934, 2062, 795, 1853, 4038, 953, 77, 3246, 503, 4927, 3715, 5639, 5058, 1284, 4905, 5154, 763, 1234, 4911, 134, 3629, 932, 4336, 1094, 3052, 3242, 671, 1315, 5032, 3716, 388, 3859, 2713, 5486, 3602, 2627, 1041, 2977, 5350, 1968, 5770, 373, 1985, 3765, 2004, 1426, 3034, 2111, 1546, 1915, 5644, 5275, 2509, 506, 1348, 299, 3789, 4785, 2836, 4581, 2413, 2225, 2515, 4009, 5636, 3766, 588, 4554, 3324, 2363, 4480, 946, 4032, 2306, 4656, 3887, 5337, 4714, 5393, 2035, 329, 779, 103, 5665, 47, 210, 2030, 4593, 1177, 3095, 2574, 1792, 5420, 1550, 2624, 2339, 816, 3592, 1660, 1569, 1379, 2455, 4555, 1273, 5631, 2530, 2961, 3571, 4391, 2365, 4330, 5244, 398, 1464, 5260, 3222, 4700, 228, 1562, 5437, 531, 3591, 4232, 1350, 3942, 888, 3788, 4685, 4282, 3225, 1056, 545, 737, 320, 4835, 4050, 2067, 2938, 4718, 1493, 208, 457, 4897, 4311, 2124, 3835, 4207, 4640, 4854, 5140, 5651, 2561, 931, 3180, 2982, 1319, 5006, 3218, 5685, 2440, 5004, 3146, 1673, 430, 1370, 3624, 35, 4616, 4199, 3050, 382, 2263, 1616, 2442, 5558, 5124, 416, 2591, 2107, 2397, 5646, 796, 5525, 2488, 1099, 8, 4696, 4352, 754, 2939, 2903, 3803, 4474, 1716, 1929, 2789, 1103, 3333, 3037, 1942, 4457, 3846, 2683, 5332, 2551, 3014, 2898, 2172, 3025, 922, 5526, 916, 2396, 2449, 5314, 4323, 2866, 5295, 1412, 3719, 1249, 4058, 3875, 318, 1747, 45, 3328, 2525, 5105, 3492, 2539, 2902, 3123, 2709, 4968, 5101, 3775, 3700, 4942, 3471, 4292, 4871, 5173, 4588, 4557, 4991, 3391, 1414, 3747, 4439, 5163, 4083, 2640, 1165, 3646, 5125, 445, 2737, 5178, 3499, 5227, 2780, 2803, 1664, 701, 5610, 399, 334, 2441, 5628, 1213, 2236, 2322, 2581, 4481, 4870, 5141, 2066, 5064, 4341, 1774, 4786, 4917, 4754, 1875, 2806, 3855, 1082, 1964, 2312, 4767, 3424, 5635, 3209, 1102, 1800, 465, 5047, 5335, 5263, 3152, 5643, 1388, 1841, 4686, 5079, 1626, 5279, 2671, 3182, 168, 1156, 1062, 2991, 2552, 270, 2802, 854, 3763, 2190, 422, 3370, 4366, 2485, 2451, 162, 4137, 3387, 5168, 355, 1422, 5475, 4861, 3054, 1766, 3851, 3839, 838, 435, 1030, 2305, 5446, 2272, 2682, 1960, 3237, 4663, 2911, 2276, 5530, 5354, 2497, 4392, 4112, 2256, 379, 2914, 691, 2294, 4658, 1572, 3358, 1311, 3918, 3005, 3939, 1681, 4407, 2743, 4025, 2699, 5242, 1470, 4872, 1696, 3911, 1018, 5663, 5563, 3538, 1658, 2268, 3158, 3425, 1974, 3767, 2881, 2242, 4304, 801, 4801, 2402, 4850, 3568, 4351, 702, 2357, 4326, 1645, 2599, 5660, 3822, 1230, 5690, 1864, 5183, 3286, 87, 3293, 3641, 4967, 1752, 2458, 956, 5181, 972, 1694, 2933, 5675, 872, 3852, 4763, 2335, 86, 2234, 1186, 1903, 4246, 3447, 4866, 5380, 1090, 3346, 1764, 4138, 3221, 4293, 293, 1137, 3127, 5146, 513, 935, 1354, 5762, 5184, 3030, 2494, 3357, 2614, 4519, 3522, 156, 2812, 4971, 5463, 1756, 5780, 2213, 1668, 5304, 464, 4414, 1304, 402, 4865, 4978, 2703, 1367, 5156, 4054, 591, 2100, 1187, 2091, 5251, 4809, 3969, 235, 60, 10, 4170, 3528, 1777, 5531, 4659, 5478, 4144, 1487, 2344, 5424, 3972, 2356, 1005, 2145, 3445, 4606, 1191, 4154, 658, 1667, 2701, 3999, 915, 1100, 5113, 1278, 3516, 676, 2019, 4100, 5766, 5732, 1394, 800, 2560, 2986, 4250, 1303, 1450, 3047, 4432, 5076, 1528, 1032, 844, 3344, 2643, 1508, 480, 3669, 2819, 2469, 2264, 79, 5084, 1533, 5442, 4003, 372, 690, 4281, 3603, 5385, 2166, 3663, 3329, 1887, 3454, 2467, 4764, 3800, 3077, 2980, 3273, 1047, 1719, 3962, 2756, 4451, 423, 2367, 1133, 1238, 5571, 3299, 313, 672, 5696, 3040, 4954, 4915, 4512, 5340, 4626, 1930, 3869, 1742, 2966, 4455, 5202, 1038, 4524, 427, 3549, 5208, 2739, 3832, 2375, 1353, 274, 1355, 1625, 4261, 2615, 4535, 1441, 1653, 4487, 3965, 2183, 2253, 548, 3756, 368, 2608, 4213, 1980, 1451, 3073, 1993, 837, 3643, 3371, 2549, 1630, 2288, 766, 1465, 3743, 3350, 375, 1604, 1092, 4367, 3671, 2209, 2369, 453, 745, 333, 5536, 3840, 4532, 4719, 5349, 3099, 3457, 3302, 4950, 4716, 1759, 508, 1122, 2665, 3109, 899, 1491, 1277, 5441, 3890, 621, 1690, 1043, 5277, 1352, 5698, 4952, 4666, 5328, 2270, 269, 257, 687, 2042, 4836, 4196, 2284, 233, 5498, 5382, 2604, 1174, 2668, 443, 4103, 2564, 519, 3923, 4286, 1361, 1006, 4753, 290, 4101, 3163, 978, 2341, 4864, 3194, 245, 2839, 1614, 3596, 2610, 4127, 230, 5297, 4678, 361, 772, 2114, 1472, 5649, 660, 4265, 1534, 5278, 1359, 3818, 2949, 3467, 3515, 3267, 3770, 1631, 3463, 2337, 7, 476, 2300, 243, 2721, 2342, 715, 3507, 460, 3072, 493, 2316, 1641, 1795, 4507, 4443, 1424, 3450, 1901, 5761, 3706, 3466, 237, 2698, 339, 4722, 5234, 944, 4841, 645, 2548, 1552, 2112, 1132, 4646, 630, 1300, 4295, 1532, 923, 4080, 2349, 4607, 4180, 3145, 4769, 3570, 4397, 2992, 5196, 5212, 2545, 521, 3261, 786, 1763, 839, 3720, 2653, 2179, 520, 708, 1731, 1554, 5038, 1525, 5490, 196, 902, 3404, 4831, 4804, 1883, 4730, 3655, 5693, 3656, 404, 4291, 2483, 879, 337, 4755, 3905, 1227, 3010, 370, 5142, 3165, 4891, 3418, 4486, 4361, 1108, 4178, 2727, 5593, 3239, 1256, 860, 189, 3223]\n",
            "[5248, 4275, 1786, 5280, 4095, 695, 4949, 4473, 5089, 3953, 634, 2638, 1640, 5598, 5509, 4688, 3512, 3797, 1245, 4085, 1845, 3245, 804, 5489, 4676, 4743, 1019, 2222, 3733, 1881, 48, 431, 1368, 3785, 1444, 2144, 317, 2259, 2327, 3689, 3608, 3400, 2964, 3627, 1928, 1669, 12, 1775, 5153, 2127, 4858, 2414, 3845, 1639, 2942, 2994, 5578, 5725, 882, 4313, 4096, 2383, 446, 4289, 2676, 2425, 3000, 1593, 4825, 2118, 4584, 3975, 4090, 5247, 2336, 1280, 2670, 669, 68, 2418, 2261, 4732, 2871, 2147, 4389, 4903, 4748, 681, 3933, 500, 5299, 1515, 789, 5500, 820, 3771, 400, 2054, 415, 803, 3944, 4559, 4765, 3560, 5303, 2099, 623, 2517, 1583, 3058, 2849, 4583, 1193, 1998, 2875, 5546, 5524, 2680, 4208, 3091, 5218, 1338, 4828, 266, 85, 4060, 3431, 2196, 5022, 2809, 2526, 5600, 4611, 1966, 3162, 1336, 1126, 325, 5404, 5592, 1291, 4362, 1477, 4660, 2592, 3506, 4567, 987, 1666, 127, 4399, 4166, 2254, 2311, 637, 550, 3746, 3340, 4315, 3889, 4745, 2957, 2661, 32, 3622, 900, 5246, 4297, 4278, 4240, 3202, 2508, 5748, 3057, 4430, 5129, 1083, 5164, 610, 3164, 957, 2822, 1347, 4231, 4766, 5648, 5002, 4321, 5110, 50, 3799, 5312, 4136, 971, 2250, 3895, 1015, 2512, 4308, 3008, 3774, 3645, 885, 3213, 1458, 4641, 1773, 2486, 4456, 3462, 4803, 3443, 3649, 4623, 2119, 5425, 4008, 2330, 5114, 2136, 2448, 3438, 2537, 4970, 2088, 4712, 3594, 1292, 2273, 1299, 5618, 2376, 3033, 3682, 2897, 5014, 1733, 5195, 3300, 4667, 2890, 5702, 1229, 2959, 4724, 4217, 296, 4021, 2602, 1622, 561, 5371, 4489, 4697, 735, 1860, 629, 4566, 3666, 4468, 1582, 5379, 2906, 2140, 5574, 181, 3837, 4233, 583, 111, 286, 5051, 5487, 1680, 459, 4132, 4822, 1516, 1606, 4948, 2302, 5093, 4885, 942, 1538, 161, 3699, 4509, 1512, 774, 4774, 2005, 1912, 5250, 2820, 1997, 2378, 673, 4359, 20, 378, 5207, 3858, 3626, 5069, 2401, 1817, 517, 1948, 4228, 813, 5308, 5703, 1044, 4030, 4016, 5253, 4396, 4494, 3616, 1084, 1520, 4094, 4105, 3355, 272, 1760, 2125, 999, 3375, 1144, 3459, 353, 2462, 2016, 2953, 4629, 1130, 3416, 1496, 2499, 1950, 3301, 2301, 3921, 3126, 4675, 4019, 5713, 5128, 1953, 657, 4543, 197, 2892, 4701, 3316, 1467, 497, 1290, 5680, 2824, 2583, 5119, 3313, 4002, 2134, 3172, 2200, 5171, 2675, 3678, 868, 1574, 4133, 2044, 1251, 770, 248, 3545, 526, 1275, 2677, 2077, 1022, 149, 5505, 4375, 305, 2374, 3518, 565, 712, 572, 1479, 3114, 984, 1027, 1244, 1294, 867, 2039, 607, 2795, 1836, 5726, 2101, 1152, 307, 3868, 4427, 5510, 5774, 3748, 3208, 4682, 644, 3281, 2632, 250, 3673, 3523, 458, 4514, 240, 491, 4093, 201, 4130, 2985, 483, 1521, 5111, 4899, 173, 577, 2371, 2038, 1045, 2554, 3393, 5625, 3744, 2905, 3513, 4043, 2355, 3407, 1688, 4298, 2435, 137, 5545, 3683, 5471, 1840, 5746, 3262, 2633, 1946, 1107, 5704, 5177, 3170, 5043, 4037, 2457, 1050, 1428, 4845, 877, 5222, 5000, 23, 4760, 107, 1584, 5755, 52, 4798, 4542, 4276, 4062, 5596, 5161, 5476, 5617, 5452, 3368, 5535, 3791, 919, 3311, 421, 4403, 688, 5288, 2692, 4220, 3336, 556, 3001, 5409, 5264, 159, 4924, 501, 2258, 2097, 5219, 2918, 4513, 2885, 4251, 2026, 1197, 2104, 613, 3768, 3500, 5700, 2018, 3248, 2956, 4984, 580, 4221, 2801, 1522, 1591, 3883, 2716, 1166, 2385, 213, 51, 1070, 5170, 3254, 4441, 2093, 3160, 1612, 5594, 2858, 5077, 2102, 1809, 4088, 2205, 3879, 4317, 297, 73, 703, 1332, 1488, 1637, 15, 2382, 5417, 2437, 3497, 538, 3390, 3124, 5653, 1096, 338, 4689, 4053, 5012, 2870, 3968, 704, 4197, 2569, 929, 5751, 783, 403, 4287, 4973, 1785, 1310, 3900, 2582, 5577, 5749, 4635, 2410, 1748, 4252, 481, 2782, 5722, 2249, 4605, 5527, 2700, 5315, 1503, 590, 3595, 1049, 3191, 878, 2578, 3947, 4259, 1000, 5427, 843, 3588, 3709, 2540, 450, 1042, 5341, 4840, 4445, 3836, 3511, 4725, 1057, 4684, 781, 1494, 883, 5133, 3115, 2746, 3946, 4893, 4176, 5745, 5414, 5453, 2808, 5104, 2996, 4102, 1878, 3907, 384, 2393, 5301, 4577, 5460, 1590, 3251, 2718, 552, 1366, 3531, 4877, 5358, 5132, 3259, 2409, 215, 3951, 2759, 1327, 5026, 3867, 1430, 4937, 2285, 1794, 5039, 3554, 911, 3735, 1851, 348, 2947, 958, 4301, 3419, 2303, 5363, 3589, 2297, 4337, 4204, 4737, 4499, 5674, 1486, 426, 4236, 252, 829, 4914, 515, 601, 762, 5670, 2406, 2364, 3070, 1105, 1652, 2110, 1549, 546, 2430, 420, 2733, 912, 753, 559, 292, 5372, 3159, 1969, 2687, 5074, 2531, 4230, 4401, 4143, 2763, 3737, 232, 1504, 4274, 5589, 3657, 3455, 2634, 2047, 3143, 3098, 1636, 206, 5231, 3927, 2923, 345, 100, 2773, 2657, 2804, 4815, 3784, 2952, 1106, 5176, 1013, 4922, 335, 4683, 3873, 5473, 3087, 3263, 1971, 1963, 1818, 3118, 3341, 1295, 2973, 2600, 3027, 4116, 1055, 492, 309, 1389, 3490, 1776, 2041, 1078, 3168, 3232, 4400, 3628, 857, 4181, 5381, 2647, 1097, 4235, 4739, 3802, 5127, 1921, 5440, 1252, 5268, 895, 5329, 767, 119, 1734, 3958, 5115, 723, 740, 3524, 4644, 5650, 3510, 3470, 678, 54, 1933, 1040, 2023, 3214, 2611, 3045, 2984, 2995, 3885, 4888, 1117, 2487, 3197, 3289, 3128, 4402, 3305, 2083, 5516, 3076, 806, 2085, 3394, 392, 3369, 4343, 1473, 5402, 4657, 1079, 1717, 5374, 282, 4185, 441, 5426, 792, 3386, 3730, 3134, 3080, 2445, 563, 3544, 1067, 566, 3440, 2226, 4192, 4285, 2983, 2473, 1232, 3547, 4444, 1071, 2416, 805, 573, 2286, 5143, 1262, 3572, 178, 2078, 2519, 4947, 2255, 2459, 4662, 1541, 92, 186, 40, 2899, 3809, 705, 1409, 5148, 3552, 2617, 1431, 5145, 482, 898, 2395, 4900, 2660, 3276, 323, 1009, 1947, 1170, 3542, 3036, 4568, 2566, 470, 3986, 1957, 2848, 858, 3015, 1439, 4602, 462, 504, 295, 5313, 1423, 2491, 1037, 963, 5418, 5669, 5466, 1638, 5318, 451, 4931, 5740, 4449, 3498, 3928, 4525, 1395, 2697, 2838, 1446, 5507, 116, 1437, 682, 4140, 4471, 2741, 3688, 5759, 4883, 1712, 76, 1393, 2155, 3380, 130, 5607, 3412, 1419, 567, 1337, 3738, 997, 4205, 3397, 3924, 1077, 3917, 1039, 1432, 1573, 5048, 4074, 4638, 4773, 455, 3551, 4018, 5647, 4795, 819, 1285, 3353, 773, 2377, 2007, 2755, 1225, 5359, 3957, 83, 4886, 3201, 821, 5673, 4959, 4187, 1199, 3153, 2685, 4916, 4243, 2679, 3654, 4335, 3234, 498, 2157, 3740, 2963, 5783, 4506, 5159, 1391, 2185, 5040, 1940, 2404, 4695, 1198, 1302, 4051, 4526, 1597, 287, 3006, 1628, 2160, 4694, 3343, 1789, 914, 5036, 1547, 5390, 2654, 3849, 2482, 321, 5552, 294, 2794, 4703, 836, 2723, 5008, 3264, 175, 4550, 4118, 1373, 2748, 1615, 496, 3483, 4987, 3828, 5013, 1801, 2006, 3465, 1167, 3871, 485, 4705, 2873, 1784, 99, 3504, 2138, 4076, 2816, 3659, 21, 2390, 5572, 5451, 4222, 5570, 2799, 2958, 5707, 471, 4812, 4383, 3807, 1075, 1184, 5575, 3018, 1413, 2011, 2228, 4082, 780, 5565, 3609, 4814, 1872, 95, 4374, 2109, 1927, 2028, 5126, 2543, 2495, 2299, 2967, 3468, 2003, 4768, 4390, 2879, 1466, 4925, 1885, 5579, 5255, 4572, 5585, 4120, 822, 3987, 4046, 951, 5569, 4161, 5028, 3667, 4727, 3664, 616, 1163, 5216, 1705, 1837, 1121, 1827, 5197, 5205, 3171, 594, 241, 4206, 4398, 571, 5461, 69, 5254, 3684, 2878, 4309, 5392, 2577, 3049, 5775, 1342, 4110, 4160, 2282, 1721, 5103, 1182, 1334, 5262, 3272, 4153, 1724, 1214, 812, 973, 3600, 5003, 1026, 5081, 4067, 5021, 4156, 3752, 2975, 5532, 143, 4477, 5191, 4856, 5229, 1627, 788, 3063, 4986, 5611, 1556, 2094, 1958, 2283, 5620, 170, 3902, 3149, 1123, 4634, 1312, 4618, 2106, 897, 5684, 5444, 27, 5622, 5194, 4868, 3870, 4372, 1698, 2968, 585, 2783, 84, 4267, 970, 3140, 1703, 4649, 976, 3250, 484, 1767, 689, 1916, 1086, 1813, 1069, 499, 1787, 4681, 2380, 1127, 5738, 4146, 2834, 2057, 4867, 312, 4851, 4599, 3931, 4531, 1931, 1834, 5496, 1648, 798, 1025, 3698, 4226, 223, 4827, 2904, 5548, 4811, 4596, 849, 3007, 5448, 2523, 1101, 3780, 5652, 5273, 4547, 1941, 395, 3148, 3101, 5157, 3997, 1898, 1896, 3081, 4733, 4237, 437, 2211, 4325, 1343, 4253, 194, 1017, 2710, 2379, 5503, 1, 5470, 3334, 4601, 4544, 541, 1116, 5699, 4670, 3154, 2628, 3280, 2506, 1661, 2689, 4654, 2177, 2607, 3829, 2765, 4190, 4492, 1629, 1104, 3238, 2082, 2504, 5642, 3757, 5517, 3577, 1460, 4859, 3764, 4356, 2965, 1602, 3189, 2201, 2601, 3308, 1750, 5215, 4624, 131, 793, 5756, 982, 602, 2590, 3593, 5249, 2219, 4504, 3535, 5015, 2790, 842, 4651, 4147, 1613, 3038, 659, 167, 1900, 2935, 2757, 558, 1601, 3108, 3278, 1415, 2468, 4111, 389, 3110, 1935, 949, 1421, 3814, 5190, 138, 2761, 4124, 2347, 3309, 3169, 2595, 5317, 1264, 3365, 3981, 1183, 109, 1548, 1888, 3915, 4437, 4191, 1146, 1824, 3224, 4679, 4479, 578, 3564, 3339, 5049, 4408, 3166, 4920, 3681, 2507, 5158, 5772, 5054, 2240, 4779, 1644, 2928, 1662, 3337, 530, 1093, 5356, 1741, 1799, 1553, 4119, 1710, 5522, 3321, 2154, 3292, 4279, 1282, 2020, 371, 5285, 1873, 3703, 4979, 222, 5130, 3107, 4848, 3617, 2146, 518, 2916, 2912, 3178, 2941, 1674, 1481, 1891, 5306, 4195, 3116, 1581, 4926, 3192, 4068, 5538, 4826, 5107, 2193, 1120, 4497, 4534, 4976, 3686, 4142, 1190, 608, 2896, 3403, 5087, 3345, 2562, 4788, 1619, 3779, 2167, 1563, 179, 4006, 3122, 393, 3787, 2433, 4539, 281, 1814, 4661, 3980, 4965, 5573, 936, 2859, 3759, 152, 4418, 2921, 3488, 4438, 5588, 4747, 4248, 848, 2279, 1212, 3103, 1320, 4966, 1568, 419, 3354, 4125, 1609, 618, 3548, 4944, 2443, 4039, 3269, 2036, 2148, 4128, 967, 5059, 5388, 4131, 5367, 1988, 5423, 2334, 3096, 4029, 2103, 3817, 1443, 2850, 4078, 182, 881, 3974, 3505, 4306, 2143, 4680, 5443, 5088, 3783, 4857, 3132, 4882, 5061, 4171, 1830, 4014, 1480, 3039, 5542, 1882, 5731, 1181, 1987, 4643, 5629, 1991, 184, 5664, 5627, 1618, 1758, 4040, 2605, 1514, 1567, 880, 363, 2069, 4091, 5290, 2304, 4219, 4592, 750, 3625, 1530, 818, 5233, 5758, 2855, 2776, 920, 3713, 2061, 344, 5605, 1771, 5100, 841, 4982, 3216, 1587, 115, 3121, 2181, 5239, 4022, 3426, 4075, 4260, 2944, 3842, 5422, 1738, 4880, 2251, 4158, 4225, 3405, 4633, 5714, 1678, 5683, 4001, 5557, 5484, 5083, 3310, 4061, 4446, 2095, 3576, 3637, 4782, 921, 3448, 3690, 3964, 5274, 847, 1469, 90, 166, 195, 3139, 5433, 3648, 4529, 5590, 3892, 3736, 747, 2068, 5351, 2071, 5482, 2717, 1270, 1959, 3501, 1889, 576, 1195, 1502, 5672, 37, 597, 2074, 2498, 3137, 4465, 4416, 4434, 1914, 3848, 4894, 1239, 5692, 3913, 1051, 1737, 4671, 3240, 239, 4579, 3069, 2060, 1996, 2463, 1978, 4589, 2636, 5344, 2389, 799, 5655, 4106, 2860, 1147, 1490, 752, 604, 1870, 1196, 992, 57, 833, 1151, 1192, 284, 961, 1605, 4516, 2174, 4269, 4574, 777, 3734, 1932, 1204, 1934, 4974, 4290, 4902, 3282, 74, 4210, 3937, 4873, 4167, 5717, 289, 4898, 2452, 933, 1014, 3749, 2022, 2271, 322, 5108, 4333, 4327, 474, 3956, 3428, 5322, 1672, 5305, 29, 2171, 198, 2559, 5352, 5348, 5138, 1810, 369, 2212, 2195, 5747, 2224, 3236, 2345, 1396, 3053, 5559, 2843, 928, 1890, 4452, 3605, 1962, 5010, 4069, 489, 4270, 2343, 1683, 4405, 3844, 5602, 1339, 1846, 264, 1324, 874, 1651, 1052, 3256, 1509, 1859, 4404, 2002, 5286, 5331, 1989, 5399, 5343, 4955, 193, 628, 2438, 1209, 4084, 3555, 438, 2869, 3384, 3853, 4515, 5586, 1143, 1531, 3217, 4901, 2466, 3587, 3722, 642, 3898, 2252, 2511, 3491, 2742, 4041, 3161, 5005, 4793, 4388, 4573, 490, 5357, 2541, 4816, 3976, 1540, 1729, 4070, 595, 4303, 2384, 4121, 5608, 5270, 1726, 4595, 734, 5355, 5459, 1449, 1713, 2314, 4108, 1708, 5773, 1060, 1478, 3460, 278, 3493, 3179, 727, 4045, 2970, 3739, 1905, 2639, 2999, 324, 2934, 3451, 586, 5597, 2164, 3707, 758, 101, 1253, 4448, 2772, 2399, 3679, 396, 2558, 1331, 2444, 2049, 64, 1455, 2505, 733, 1992, 1397, 4249, 155, 424, 2113, 1828, 3032, 4229, 1874, 1358, 98, 302, 1323, 231, 2874, 124, 884, 891, 1646, 5162, 598, 2880, 1335, 2788, 1207, 3229, 3031, 2194, 376, 2373, 927, 1689, 3360, 2745, 2575, 67, 2962, 276, 2856, 91, 2245, 1825, 592, 655, 1671, 4491, 5387, 4334, 5311, 1265, 5192, 1194, 1919, 5360, 5017, 3718, 2158, 5712, 4177, 3830, 656, 1918, 4778, 2889, 1016, 3413, 3480, 2876, 4005, 3446, 4254, 5767, 5067, 1089, 16, 5090, 3307, 4830, 2502, 2752, 4036, 5716, 1990, 4277, 5121, 3094, 3532, 1589, 5377, 917, 1543, 2188, 606, 4963, 2208, 5472, 907, 1482, 652, 1819, 4761, 327, 5, 5050, 5710, 3125, 5679, 5554, 203, 1314, 3831, 2275, 3647, 2045, 2847, 1617, 3068, 2690, 2087, 2635, 1757, 4056, 2872, 308, 2105, 4713, 1871, 4990, 4353, 3916, 5099, 414, 2398, 5449, 5225, 4411, 4320, 4562, 2420, 364, 340, 2076, 4691, 853, 3176, 4561, 1031, 4625, 2292, 3104, 3348, 3129, 3537, 1855, 4738, 3677, 216, 5368, 3821, 2053, 560, 3258, 3732, 5030, 509, 4193, 3284, 2997, 2388, 555, 4394, 3464, 3092, 3970, 977, 826, 4527, 516, 255, 553, 3599, 172, 3893, 893, 1679, 3994, 3211, 5688, 1780, 5705, 3723, 5416, 3823, 1297, 2121, 771, 5011, 5109, 2175, 2176, 4548, 4165, 5139, 2659, 3439, 1798, 190, 3332, 3117, 139, 1489, 4299, 4775, 4847, 4319, 1461, 3401, 5739, 2223, 1911, 117, 22, 3366, 2565, 4417, 5604, 5375, 5626, 4064, 2722, 4571, 3086, 5073, 1142, 2315, 809, 3533, 2189, 1704, 3708, 4440, 3995, 199, 2720, 5555, 1695, 2900, 17, 5037, 449, 4746, 1410, 890, 2513, 261, 1739, 151, 3017, 3088, 4266, 2940, 3991, 5429, 2180, 1877, 2333, 3485, 5282, 3530, 4175, 1656, 5454, 746, 1237, 2313, 3909, 4459, 3135, 2857, 1880, 3943, 4994, 1832, 126, 3926, 2656, 3877, 5566, 1842, 2257, 4912, 4958, 1168, 2584, 2749, 643, 2570, 5310, 2204, 1684, 626, 3819, 2666, 2417, 2930, 722, 3550, 271, 5595, 3021, 1779, 2835, 5619, 3726, 3482, 1020, 452, 4706, 2766, 5539, 4560, 3696, 3230, 3565, 2280, 3983, 744, 5754, 1564, 316, 3105, 3668, 3776, 2265, 19, 864, 1374, 2159, 3356, 5682, 3862, 2317, 5757, 1595, 43, 5616, 3978, 3973, 1557, 71, 1725, 3085, 1560, 2248, 3112, 3012, 759, 1849, 4578, 5316, 105, 4332, 2929, 1088, 2400, 3729, 823, 391, 5066, 3527, 3590, 25, 4500, 4031, 336, 3777, 3614, 5477, 2405, 5338, 3794, 1254, 4381, 3847, 2863, 1954, 1399, 2603, 432, 5241, 183, 815, 2784, 1701, 2663, 53, 3922, 1351, 4498, 3363, 3754, 1576, 3936, 713, 887, 1867, 3880, 1843, 3731, 3903, 2658, 4921, 1129, 2830, 5719, 436, 5662, 2842, 4690, 2586, 3247, 5512, 910, 4344, 570, 4365, 622, 1004, 1145, 1269, 4565, 3623, 1381, 699, 834, 106, 996, 3062, 1442, 2825, 1033, 2439, 3929, 2550, 311, 3598, 1153, 4066, 386, 3796, 3728, 479, 3912, 3841, 4284, 1235, 909, 5172, 4975, 2618, 5621, 5479, 662, 1743, 3938, 2090, 2750, 4201, 44, 4879, 1816, 986, 3881, 1803, 401, 4938, 1523, 4997, 905, 1714, 360, 3458, 3993, 4245, 5556, 1154, 4244, 875, 1624, 263, 1826, 4280, 5582, 4305, 5736, 3508, 5373, 2098, 2798, 5528, 620, 158, 3574, 2481, 1113, 2516, 4655, 3473, 1325, 418, 790, 4813, 3372, 36, 3318, 4409, 4645, 454, 1220, 118, 3097, 2775, 840, 2115, 2407, 5465, 4918, 4896, 3888, 5070, 4410, 2695, 466, 894, 1575, 2428, 4023, 448, 3825, 4792, 5428, 4182, 2922, 4376, 5455, 5086, 1468, 4145, 1805, 4216, 1157, 1248, 4609, 3061, 1580, 5549, 2730, 535, 603, 4188, 4693, 1411, 1034, 3914, 4027, 4797, 2308, 5721, 1657, 2589, 1306, 1267, 2153, 5378, 5398, 5024, 725, 3131, 998, 4354, 554, 1171, 4904, 1676, 4852, 385, 1897, 2278, 3882, 2173, 4907, 5656, 5319, 5781, 5152, 1001, 856, 4026, 1118, 1053, 2669, 4541, 3093, 5272, 2461, 3294, 4413, 4772, 1598, 2823, 2546, 3632, 3295, 1474, 1058, 532, 866, 5027, 4010, 38, 4466, 242, 2573, 3190, 5155, 557, 5782, 5271, 2888, 4552, 3327, 72, 13, 2489, 889, 2767, 5182, 2126, 1011, 4476, 3048, 4123, 5623, 4603, 3585, 5768, 5659, 1854, 315, 2239, 4339, 153, 1376, 859, 1139, 562, 568, 2122, 4972, 5238, 3826, 4817, 5403, 2708, 4063, 408, 5160, 1943, 3613, 1850, 4485, 1305, 1224, 2203, 536, 4951, 2837, 1241, 3351, 2014, 1687, 1838, 351, 3274, 3271, 1529, 738, 2295, 2117, 5638, 505, 246, 4884, 4742, 4194, 3102, 947, 4729, 4752, 1561, 3866, 2232, 1211, 4385, 397, 2476, 4787, 2081, 2895, 4613, 1061, 4687, 1831, 614, 2350, 3610, 3210, 3227, 1459, 4209, 1862, 4744, 5071, 757, 2576, 5092, 4820, 1599, 4168, 1456, 2760, 4810, 381, 1405, 2976, 3427, 3020, 475, 2321, 3813, 5258, 5118, 1218, 4371, 4875, 11, 636, 1936, 1507, 3996, 4723, 1899, 5431, 5541, 3326, 4129, 5347, 5221, 5483, 4406, 407, 3306, 145, 2137, 2500, 3378, 3029, 2917, 3478, 275, 1476, 4347, 4839, 650, 1879, 2791, 273, 5384, 5330, 1091, 3795, 3891, 1131, 3347, 4910, 593, 314, 4704, 1387, 5386, 3212, 4072, 2826, 128, 2214, 4424, 784, 1858, 260, 3408, 3833, 1360, 3559, 4369, 3810, 4395, 1524, 288, 4122, 2998, 1128, 1416, 356, 135, 1944, 1236, 2033, 4087, 1216, 3417, 3725, 2688, 635, 3253, 3894, 2490, 1720, 5550, 980, 473, 529, 5137, 1293, 4433, 3569, 4582, 1307, 3437, 945, 4664, 3352, 5562, 2810, 3421, 3389, 3982, 4608, 1063, 564, 924, 136, 5508, 2362, 1175, 3304, 46, 2622, 4628, 5292, 2691, 4790, 3949, 3349, 4200, 599, 714, 5493, 1501, 2426, 3411, 4447, 487, 811, 5217, 1188, 2553, 1068, 3955, 4134, 1868, 1892, 5695, 721, 5362, 1054, 4428, 4179, 2332, 2813, 2769, 3940, 1447, 3979, 5261, 5085, 300, 234, 1333, 1180, 4012, 3317, 4493, 5046, 1709, 2216, 1539, 3331, 5438, 3675, 5708, 2524, 686, 4355, 4264, 3786, 1517, 1923, 1566, 4853, 1603, 4461, 2199, 200, 4570, 5082, 5407, 4378, 5641, 5336, 4218, 4157, 2197, 3430, 365, 1272, 1665, 2169, 2051, 1857, 5276, 2381, 1185, 683, 1356, 4239, 1769, 979, 3338, 5033, 169, 2075, 4501, 3863, 5353, 125, 3662, 4929, 1404, 3945, 2319, 4575, 3026, 1007, 3760, 5211, 89, 4585, 5213, 3422, 1255, 4789, 4238, 1979, 742, 34, 1243, 4126, 2471, 5096, 3584, 157, 651, 2644, 1257, 1008, 5230, 1994, 4257, 1608, 2424, 4855, 2142, 3044, 4736, 1945, 1869, 3615, 1035, 1635, 5223, 1098, 5187, 4086, 2950]\n",
            "<class 'list'>\n",
            "Train Dataset: 2892\n",
            "Valid Dataset: 2892\n",
            "Test Dataset: 2893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJVoYy4VPZOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_classes = np.zeros(101)\n",
        "\n",
        "for elem in train_dataset:\n",
        "  train_classes[elem[1]] += 1\n",
        "\n",
        "val_classes = np.zeros(101)\n",
        "\n",
        "for elem in val_dataset:\n",
        "  val_classes[elem[1]] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGIsCYQlcH3V",
        "colab_type": "code",
        "outputId": "42e347e5-e4c6-4c9e-f0b9-9aa4fe96e2f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "print(train_classes)\n",
        "ax = sns.barplot(x=np.linspace(0, 100, num=101), y=train_classes)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 18. 267.  11.  11.  16.  24.  20.  13.  42.  29.  14.  35.  26.  13.\n",
            "  14.  40.  15.  19.  22.  41.  19.  25.  23.  22.  17.  12.  19.  22.\n",
            "  17.  28.  25.  19.  16.  17.  24.  28. 142. 157.  26.  25.  21.  10.\n",
            "  12.  19.  31.  34.  14.  19.  33.  19.  11.  23.  33.  37.  16.  28.\n",
            "  69.  29.  12.  28.  15.  12.  28.  12.  25. 235.  14.  11.  15.  17.\n",
            "  15.  14.  14.  13.  20.  27.  20.  17.  13.  22.  15.  28.  20.  14.\n",
            "  20.  15.  24.  20.  28.  10.  28.  17.  24.  25.  79.  10.  20.  13.\n",
            "  17.  16.  19.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYEDQ7Z21ldN",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VriRw8SI1nle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbZ1t5Qs2z4j",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exHUjtXa22DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = alexnet() # Loading AlexNet model\n",
        "\n",
        "# AlexNet has 1000 output neurons, corresponding to the 1000 ImageNet's classes\n",
        "# We need 101 outputs for Caltech-101\n",
        "net.classifier[6] = nn.Linear(4096, NUM_CLASSES) # nn.Linear in pytorch is a fully connected layer\n",
        "                                                 # The convolutional layer is nn.Conv2d\n",
        "\n",
        "# We just changed the last layer of AlexNet with a new fully connected layer with 101 outputs\n",
        "# It is strongly suggested to study torchvision.models.alexnet source code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEyL3H_R4qCf",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sjq00G94tSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "\n",
        "# Choose parameters to optimize\n",
        "# To access a different set of parameters, you have to access submodules of AlexNet\n",
        "# (nn.Module objects, like AlexNet, implement the Composite Pattern)\n",
        "# e.g.: parameters of the fully connected layers: net.classifier.parameters()\n",
        "# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) \n",
        "parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet\n",
        "\n",
        "# Define optimizer\n",
        "# An optimizer updates the weights based on loss\n",
        "# We use SGD with momentum\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Define scheduler\n",
        "# A scheduler dynamically changes learning rate\n",
        "# The most common schedule is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxYUli9d9uYQ",
        "colab_type": "text"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcoQ5fD49yT_",
        "colab_type": "code",
        "outputId": "67b3d729-8f89-412b-8134-5a98339edb6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# By default, everything is loaded to cpu\n",
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "current_step = 0\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
        "\n",
        "  # Iterate over the dataset\n",
        "  for images, labels in train_dataloader:\n",
        "    # Bring data over the device of choice\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    net.train() # Sets module in training mode\n",
        "\n",
        "    # PyTorch, by default, accumulates gradients after each backward pass\n",
        "    # We need to manually set the gradients to zero before starting a new iteration\n",
        "    optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "    # Forward pass to the network\n",
        "    outputs = net(images)\n",
        "\n",
        "    # Compute loss based on output and ground truth\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Log loss\n",
        "    if current_step % LOG_FREQUENCY == 0:\n",
        "      print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "    # Compute gradients for each layer and update weights\n",
        "    loss.backward()  # backward pass: computes gradients\n",
        "    optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "    current_step += 1\n",
        "\n",
        "  # Step the scheduler\n",
        "  scheduler.step() "
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/100, LR = [0.001]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 0, Loss 4.623844146728516\n",
            "Step 10, Loss 4.623544216156006\n",
            "Starting epoch 2/100, LR = [0.001]\n",
            "Step 20, Loss 4.619775772094727\n",
            "Starting epoch 3/100, LR = [0.001]\n",
            "Step 30, Loss 4.617133140563965\n",
            "Starting epoch 4/100, LR = [0.001]\n",
            "Step 40, Loss 4.61474609375\n",
            "Starting epoch 5/100, LR = [0.001]\n",
            "Step 50, Loss 4.61157751083374\n",
            "Starting epoch 6/100, LR = [0.001]\n",
            "Step 60, Loss 4.608094215393066\n",
            "Starting epoch 7/100, LR = [0.001]\n",
            "Step 70, Loss 4.604264736175537\n",
            "Starting epoch 8/100, LR = [0.001]\n",
            "Step 80, Loss 4.6015214920043945\n",
            "Starting epoch 9/100, LR = [0.001]\n",
            "Step 90, Loss 4.599964141845703\n",
            "Starting epoch 10/100, LR = [0.001]\n",
            "Step 100, Loss 4.602491855621338\n",
            "Starting epoch 11/100, LR = [0.001]\n",
            "Step 110, Loss 4.591472625732422\n",
            "Step 120, Loss 4.58869743347168\n",
            "Starting epoch 12/100, LR = [0.001]\n",
            "Step 130, Loss 4.590440273284912\n",
            "Starting epoch 13/100, LR = [0.001]\n",
            "Step 140, Loss 4.576876163482666\n",
            "Starting epoch 14/100, LR = [0.001]\n",
            "Step 150, Loss 4.582828521728516\n",
            "Starting epoch 15/100, LR = [0.001]\n",
            "Step 160, Loss 4.578462600708008\n",
            "Starting epoch 16/100, LR = [0.001]\n",
            "Step 170, Loss 4.577406406402588\n",
            "Starting epoch 17/100, LR = [0.001]\n",
            "Step 180, Loss 4.562382221221924\n",
            "Starting epoch 18/100, LR = [0.001]\n",
            "Step 190, Loss 4.567396640777588\n",
            "Starting epoch 19/100, LR = [0.001]\n",
            "Step 200, Loss 4.559589862823486\n",
            "Starting epoch 20/100, LR = [0.001]\n",
            "Step 210, Loss 4.552082538604736\n",
            "Starting epoch 21/100, LR = [0.001]\n",
            "Step 220, Loss 4.5362653732299805\n",
            "Step 230, Loss 4.517669200897217\n",
            "Starting epoch 22/100, LR = [0.001]\n",
            "Step 240, Loss 4.494683265686035\n",
            "Starting epoch 23/100, LR = [0.001]\n",
            "Step 250, Loss 4.478915691375732\n",
            "Starting epoch 24/100, LR = [0.001]\n",
            "Step 260, Loss 4.398067474365234\n",
            "Starting epoch 25/100, LR = [0.001]\n",
            "Step 270, Loss 4.252501010894775\n",
            "Starting epoch 26/100, LR = [0.001]\n",
            "Step 280, Loss 4.233943462371826\n",
            "Starting epoch 27/100, LR = [0.001]\n",
            "Step 290, Loss 4.268763065338135\n",
            "Starting epoch 28/100, LR = [0.001]\n",
            "Step 300, Loss 4.301709175109863\n",
            "Starting epoch 29/100, LR = [0.001]\n",
            "Step 310, Loss 4.247500896453857\n",
            "Starting epoch 30/100, LR = [0.001]\n",
            "Step 320, Loss 4.19032096862793\n",
            "Starting epoch 31/100, LR = [0.001]\n",
            "Step 330, Loss 4.250657558441162\n",
            "Step 340, Loss 4.100369453430176\n",
            "Starting epoch 32/100, LR = [0.001]\n",
            "Step 350, Loss 4.246468544006348\n",
            "Starting epoch 33/100, LR = [0.001]\n",
            "Step 360, Loss 4.308850288391113\n",
            "Starting epoch 34/100, LR = [0.001]\n",
            "Step 370, Loss 4.158793926239014\n",
            "Starting epoch 35/100, LR = [0.001]\n",
            "Step 380, Loss 4.214175701141357\n",
            "Starting epoch 36/100, LR = [0.001]\n",
            "Step 390, Loss 4.192922592163086\n",
            "Starting epoch 37/100, LR = [0.001]\n",
            "Step 400, Loss 4.160589218139648\n",
            "Starting epoch 38/100, LR = [0.001]\n",
            "Step 410, Loss 4.316192150115967\n",
            "Starting epoch 39/100, LR = [0.001]\n",
            "Step 420, Loss 4.242664337158203\n",
            "Starting epoch 40/100, LR = [0.001]\n",
            "Step 430, Loss 4.230734348297119\n",
            "Starting epoch 41/100, LR = [0.001]\n",
            "Step 440, Loss 4.199349403381348\n",
            "Step 450, Loss 4.178070545196533\n",
            "Starting epoch 42/100, LR = [0.001]\n",
            "Step 460, Loss 4.169425010681152\n",
            "Starting epoch 43/100, LR = [0.001]\n",
            "Step 470, Loss 4.175204753875732\n",
            "Starting epoch 44/100, LR = [0.001]\n",
            "Step 480, Loss 4.199365139007568\n",
            "Starting epoch 45/100, LR = [0.001]\n",
            "Step 490, Loss 4.139002799987793\n",
            "Starting epoch 46/100, LR = [0.001]\n",
            "Step 500, Loss 4.239612102508545\n",
            "Starting epoch 47/100, LR = [0.001]\n",
            "Step 510, Loss 4.125704765319824\n",
            "Starting epoch 48/100, LR = [0.001]\n",
            "Step 520, Loss 4.2167792320251465\n",
            "Starting epoch 49/100, LR = [0.001]\n",
            "Step 530, Loss 4.163145542144775\n",
            "Starting epoch 50/100, LR = [0.001]\n",
            "Step 540, Loss 4.168559551239014\n",
            "Starting epoch 51/100, LR = [0.001]\n",
            "Step 550, Loss 4.027056694030762\n",
            "Step 560, Loss 4.141206741333008\n",
            "Starting epoch 52/100, LR = [0.001]\n",
            "Step 570, Loss 3.959251880645752\n",
            "Starting epoch 53/100, LR = [0.001]\n",
            "Step 580, Loss 4.025550842285156\n",
            "Starting epoch 54/100, LR = [0.001]\n",
            "Step 590, Loss 3.8835315704345703\n",
            "Starting epoch 55/100, LR = [0.001]\n",
            "Step 600, Loss 4.006525993347168\n",
            "Starting epoch 56/100, LR = [0.001]\n",
            "Step 610, Loss 4.101348400115967\n",
            "Starting epoch 57/100, LR = [0.001]\n",
            "Step 620, Loss 3.871371269226074\n",
            "Starting epoch 58/100, LR = [0.001]\n",
            "Step 630, Loss 3.9555373191833496\n",
            "Starting epoch 59/100, LR = [0.001]\n",
            "Step 640, Loss 3.9428799152374268\n",
            "Starting epoch 60/100, LR = [0.001]\n",
            "Step 650, Loss 4.093968868255615\n",
            "Starting epoch 61/100, LR = [0.001]\n",
            "Step 660, Loss 3.8631696701049805\n",
            "Step 670, Loss 3.9113969802856445\n",
            "Starting epoch 62/100, LR = [0.001]\n",
            "Step 680, Loss 3.8583579063415527\n",
            "Starting epoch 63/100, LR = [0.001]\n",
            "Step 690, Loss 4.061176776885986\n",
            "Starting epoch 64/100, LR = [0.001]\n",
            "Step 700, Loss 3.9978654384613037\n",
            "Starting epoch 65/100, LR = [0.001]\n",
            "Step 710, Loss 3.9207870960235596\n",
            "Starting epoch 66/100, LR = [0.001]\n",
            "Step 720, Loss 3.7483584880828857\n",
            "Starting epoch 67/100, LR = [0.001]\n",
            "Step 730, Loss 3.804222583770752\n",
            "Starting epoch 68/100, LR = [0.001]\n",
            "Step 740, Loss 3.8027942180633545\n",
            "Starting epoch 69/100, LR = [0.001]\n",
            "Step 750, Loss 3.8659636974334717\n",
            "Starting epoch 70/100, LR = [0.001]\n",
            "Step 760, Loss 3.7717442512512207\n",
            "Starting epoch 71/100, LR = [0.001]\n",
            "Step 770, Loss 3.8014702796936035\n",
            "Step 780, Loss 3.813425302505493\n",
            "Starting epoch 72/100, LR = [0.001]\n",
            "Step 790, Loss 3.9185824394226074\n",
            "Starting epoch 73/100, LR = [0.001]\n",
            "Step 800, Loss 3.8594894409179688\n",
            "Starting epoch 74/100, LR = [0.001]\n",
            "Step 810, Loss 3.8071978092193604\n",
            "Starting epoch 75/100, LR = [0.001]\n",
            "Step 820, Loss 3.6113057136535645\n",
            "Starting epoch 76/100, LR = [0.001]\n",
            "Step 830, Loss 3.6983213424682617\n",
            "Starting epoch 77/100, LR = [0.001]\n",
            "Step 840, Loss 3.721259117126465\n",
            "Starting epoch 78/100, LR = [0.001]\n",
            "Step 850, Loss 3.773883104324341\n",
            "Starting epoch 79/100, LR = [0.001]\n",
            "Step 860, Loss 3.72818922996521\n",
            "Starting epoch 80/100, LR = [0.001]\n",
            "Step 870, Loss 3.7906429767608643\n",
            "Starting epoch 81/100, LR = [0.001]\n",
            "Step 880, Loss 3.846529722213745\n",
            "Step 890, Loss 3.6862552165985107\n",
            "Starting epoch 82/100, LR = [0.001]\n",
            "Step 900, Loss 3.6874446868896484\n",
            "Starting epoch 83/100, LR = [0.001]\n",
            "Step 910, Loss 3.7019145488739014\n",
            "Starting epoch 84/100, LR = [0.001]\n",
            "Step 920, Loss 3.8194901943206787\n",
            "Starting epoch 85/100, LR = [0.001]\n",
            "Step 930, Loss 3.6280434131622314\n",
            "Starting epoch 86/100, LR = [0.001]\n",
            "Step 940, Loss 3.729531764984131\n",
            "Starting epoch 87/100, LR = [0.001]\n",
            "Step 950, Loss 3.5468742847442627\n",
            "Starting epoch 88/100, LR = [0.001]\n",
            "Step 960, Loss 3.4805707931518555\n",
            "Starting epoch 89/100, LR = [0.001]\n",
            "Step 970, Loss 3.6556482315063477\n",
            "Starting epoch 90/100, LR = [0.001]\n",
            "Step 980, Loss 3.671494245529175\n",
            "Starting epoch 91/100, LR = [1e-05]\n",
            "Step 990, Loss 3.532360553741455\n",
            "Step 1000, Loss 3.7574474811553955\n",
            "Starting epoch 92/100, LR = [0.0001]\n",
            "Step 1010, Loss 3.5744645595550537\n",
            "Starting epoch 93/100, LR = [0.0001]\n",
            "Step 1020, Loss 3.5716381072998047\n",
            "Starting epoch 94/100, LR = [0.0001]\n",
            "Step 1030, Loss 3.5482823848724365\n",
            "Starting epoch 95/100, LR = [0.0001]\n",
            "Step 1040, Loss 3.5288681983947754\n",
            "Starting epoch 96/100, LR = [0.0001]\n",
            "Step 1050, Loss 3.552137851715088\n",
            "Starting epoch 97/100, LR = [0.0001]\n",
            "Step 1060, Loss 3.645582675933838\n",
            "Starting epoch 98/100, LR = [0.0001]\n",
            "Step 1070, Loss 3.550745725631714\n",
            "Starting epoch 99/100, LR = [0.0001]\n",
            "Step 1080, Loss 3.5171871185302734\n",
            "Starting epoch 100/100, LR = [0.0001]\n",
            "Step 1090, Loss 3.698000431060791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsHFI-GAJd69",
        "colab_type": "text"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO3HV5pqJg1o",
        "colab_type": "code",
        "outputId": "61d14bcf-d320-4f95-817e-fe7b306fea58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(val_dataloader):\n",
        "  images = images.to(DEVICE)\n",
        "  labels = labels.to(DEVICE)\n",
        "\n",
        "  # Forward Pass\n",
        "  outputs = net(images)\n",
        "\n",
        "  # Get predictions\n",
        "  _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "  # Update Corrects\n",
        "  running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = running_corrects / float(len(val_dataset))\n",
        "\n",
        "print('Validation Accuracy: {}'.format(accuracy))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 12/12 [00:10<00:00,  1.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.25138312586445366\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxekmR745ySe",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSHcUqLB5yWO",
        "colab_type": "code",
        "outputId": "057f28a6-c5d5-47ad-c351-bfc2798a329a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(test_dataloader):\n",
        "  images = images.to(DEVICE)\n",
        "  labels = labels.to(DEVICE)\n",
        "\n",
        "  # Forward Pass\n",
        "  outputs = net(images)\n",
        "\n",
        "  # Get predictions\n",
        "  _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "  # Update Corrects\n",
        "  running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "print('Test Accuracy: {}'.format(accuracy))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 12/12 [00:11<00:00,  1.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.24403733148980297\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}